{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VOeM5FEyeIA",
        "outputId": "45d41908-9d24-441a-fe16-b77ae39956c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCHNTYiYyeIC"
      },
      "outputs": [],
      "source": [
        "url='https://raw.githubusercontent.com/jacob-hansen/NLP_in_EHR_2022/910d9f0fcfeab083dff53ea2e2969c175cc816a0/train.csv'\n",
        "train_df = pd.read_csv(url)\n",
        "# train_df = pd.read_csv('data/train_even.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afhdEMujyeID"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "#loading the english language small model of spacy\n",
        "en = spacy.load('en_core_web_sm')\n",
        "sw_spacy = en.Defaults.stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wj45W58YyeID"
      },
      "outputs": [],
      "source": [
        "def custom_tokenizer(sentence):\n",
        "    # lower case\n",
        "    sentence = sentence.lower()\n",
        "    # split by the label/sentence separator\n",
        "    sents = sentence.split('. val is ')\n",
        "    # seperate out the label and the next sentence\n",
        "    # print(sents)\n",
        "    new_sents = [sents[0][13:],]\n",
        "    for sent in sents[1:]:\n",
        "        new_sents.extend(sents[1].split('. ', 1))\n",
        "    # print(sents)\n",
        "    # sents = [sents[0]] + [i for i in sents[1].split('. ')]\n",
        "    # remove any trailing whitespace\n",
        "    sents = [i.strip() for i in new_sents]\n",
        "    \n",
        "    # remove stop words in every 1,3,5... sentence\n",
        "    # and apply tokenization\n",
        "    for i in range(0, len(sents), 2):\n",
        "        # remove stop words\n",
        "        sents[i] = [word for word in word_tokenize(sents[i]) if word not in sw_spacy]\n",
        "    # remove last . from the last sentence\n",
        "    sents[-1] = sents[-1][:-1]\n",
        "    \n",
        "    # flatten sents \n",
        "    return_sents = []\n",
        "    for i in range(len(sents)):\n",
        "        if i % 2 == 0:\n",
        "            return_sents.extend(sents[i])\n",
        "        else:\n",
        "            return_sents.append(sents[i])\n",
        "    # repeat the label 3 times (loc = 1,3,5..) for every sentence \n",
        "    final_sents = []\n",
        "    for i in range(0, len(return_sents)):\n",
        "        if i % 2 == 0:\n",
        "            final_sents.append(return_sents[i])\n",
        "        else:\n",
        "            final_sents.append(return_sents[i])\n",
        "            final_sents.append(return_sents[i])\n",
        "            final_sents.append(return_sents[i])\n",
        "\n",
        "    return return_sents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIERsUSRyeIE"
      },
      "outputs": [],
      "source": [
        "# apply the custom tokenizer to the dataframe\n",
        "train_df['tokenized'] = train_df['X_train'].apply(custom_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5DM-8CwyeIE",
        "outputId": "cce6f253-6893-48a6-f1f3-7fed4881d29c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"feature is indicator for prior disabilities that the patient had. mark 1 if any disabilities were noted medically.. val is -0.248.  feature is how long a person is in an unresponsive state after an injury. measured in hours.. val is 0.579.  feature is the amount of time between heart beats measured by ms between heartbeats. . val is 0.33.  feature is concentration of the tau protein in cerebrospinal fluid. measured in pg/ml\\n. val is -1.001.  feature is a patient's indication 1-10 of their day to day happiness and satisfaction.. val is -0.535.  feature is how body fat is distributed and how much of someone's body is body fat measured by bmi. range [1-3] where each number is associated to the obesity class that based on which range the bmi falls under. val is -0.191.  feature is how long, in hours, that have passed since the initial injury.. val is -0.176.  feature is how severe the patient's acne appears. 1-10 for the number of pimples or rashes across the face and chest.. val is -0.13.  feature is a head injury can either be open, where a victim suffers from damage to their brain and skull or bone structure, or closed, where there is no damage to the skull or bone structure, but rather internal brain bleeding. val is -0.824.  feature is arm pain that ranges from manageable to severe measured by asking the patient. external signs to arm injury include bleeding, protruding bone, or deformity measured by visible injury. val is 0.061. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df.iloc[90, 0].lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjv_Z7sDyeIF",
        "outputId": "a072938f-4345-489c-b245-3283b1694aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dicator',\n",
              " 'prior',\n",
              " 'disabilities',\n",
              " 'patient',\n",
              " '.',\n",
              " 'mark',\n",
              " '1',\n",
              " 'disabilities',\n",
              " 'noted',\n",
              " 'medically',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours',\n",
              " '.',\n",
              " '-0.248',\n",
              " 'feature',\n",
              " 'long',\n",
              " 'person',\n",
              " 'unresponsive',\n",
              " 'state',\n",
              " 'injury',\n",
              " '.',\n",
              " 'measured',\n",
              " 'hours']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "custom_tokenizer(train_df.iloc[90, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQvWi-syeIF"
      },
      "outputs": [],
      "source": [
        "# rename columns to X_train, y_train, and tokenized\n",
        "# train_df = train_df.rename(columns={'0': 'X_train', '1': 'y_train', 'tokenized': 'tokenized'})\n",
        "# train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Attention Layer"
      ],
      "metadata": {
        "id": "5fbzieaD4-EI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "metadata": {
        "id": "YMqItIp349kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11cTuBEyeIG"
      },
      "source": [
        "# Training RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU1nvwLcyeIH"
      },
      "outputs": [],
      "source": [
        "# import packages for training RNN\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neoe6Lv4yeIH"
      },
      "outputs": [],
      "source": [
        "target = train_df['y_train'].values\n",
        "target = to_categorical(target)\n",
        "# convert train_df['tokenized'] to a tensor\n",
        "# and pad the sequences to be the same length\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_df['tokenized'].values)\n",
        "X = tokenizer.texts_to_sequences(train_df['tokenized'].values)\n",
        "X = pad_sequences(X, maxlen=max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veBtLS3DyeIH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYYhYBT_yeII"
      },
      "outputs": [],
      "source": [
        "# #create RNN model \n",
        "# model = Sequential()\n",
        "# model.add(Embedding(10000, 128, input_length=X.shape[1]))\n",
        "# model.add(SpatialDropout1D(0.4))\n",
        "# model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(2, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create RNN model with attention\n",
        "inputs = Input((X.shape[1],))\n",
        "x = Embedding(10000, 128, input_length=X.shape[1])(inputs)\n",
        "att_in = LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(x)\n",
        "att_out=attention()(att_in)\n",
        "outputs = Dense(2, activation='softmax',trainable=True)(att_out)\n",
        "model = Model(inputs , outputs)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "etV-fqoitcg2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwfsEyXpBu2q",
        "outputId": "91e6ccb2-b500-46a4-eddb-694677181ea8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_13 (InputLayer)       [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding_16 (Embedding)    (None, 200, 128)          1280000   \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 200, 128)          131584    \n",
            "                                                                 \n",
            " attention_9 (attention)     (None, 128)               328       \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,412,170\n",
            "Trainable params: 1,412,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hTFm1O3eyeII",
        "outputId": "6fd4d6e7-cc03-45ee-e23c-830ddeccd421",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "113/113 [==============================] - 86s 737ms/step - loss: 0.5124 - accuracy: 0.8012 - val_loss: 0.5171 - val_accuracy: 0.7912\n",
            "Epoch 2/10\n",
            "113/113 [==============================] - 82s 723ms/step - loss: 0.4912 - accuracy: 0.8049 - val_loss: 0.5117 - val_accuracy: 0.7912\n",
            "Epoch 3/10\n",
            "113/113 [==============================] - 86s 763ms/step - loss: 0.4657 - accuracy: 0.8093 - val_loss: 0.5114 - val_accuracy: 0.7925\n",
            "Epoch 4/10\n",
            "113/113 [==============================] - 85s 750ms/step - loss: 0.3709 - accuracy: 0.8326 - val_loss: 0.5660 - val_accuracy: 0.7487\n",
            "Epoch 5/10\n",
            "113/113 [==============================] - 83s 733ms/step - loss: 0.3090 - accuracy: 0.8576 - val_loss: 0.6710 - val_accuracy: 0.7437\n",
            "Epoch 6/10\n",
            "113/113 [==============================] - 82s 725ms/step - loss: 0.2881 - accuracy: 0.8647 - val_loss: 0.7111 - val_accuracy: 0.7038\n",
            "Epoch 7/10\n",
            "113/113 [==============================] - 83s 733ms/step - loss: 0.2690 - accuracy: 0.8671 - val_loss: 0.7438 - val_accuracy: 0.7613\n",
            "Epoch 8/10\n",
            "113/113 [==============================] - 81s 720ms/step - loss: 0.2536 - accuracy: 0.8747 - val_loss: 0.7932 - val_accuracy: 0.7500\n",
            "Epoch 9/10\n",
            "113/113 [==============================] - 82s 729ms/step - loss: 0.2479 - accuracy: 0.8736 - val_loss: 0.8191 - val_accuracy: 0.7600\n",
            "Epoch 10/10\n",
            "113/113 [==============================] - 81s 718ms/step - loss: 0.2350 - accuracy: 0.8810 - val_loss: 0.8468 - val_accuracy: 0.7675\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(K.eval(model.optimizer.lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cw8HoGCn3VtW",
        "outputId": "6a2a5c03-1111-41e3-fd71-84010e114714"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "PAVT5P7kyeIJ",
        "outputId": "821c46bb-900a-4a6a-d5df-5a95e7012edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 - 3s - loss: 0.9237 - accuracy: 0.7610 - 3s/epoch - 108ms/step\n",
            "score: 0.92\n",
            "acc: 0.76\n"
          ]
        }
      ],
      "source": [
        "# test the model\n",
        "score, acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = 64)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Y4Jj7bnbyeIJ",
        "outputId": "4cc24a4c-5e5c-48ba-9f2c-c378900abd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 5s 70ms/step\n",
            "[[8.2434094e-01 1.7565908e-01]\n",
            " [9.9999827e-01 1.6363542e-06]\n",
            " [9.9999088e-01 9.0722951e-06]\n",
            " ...\n",
            " [7.2008241e-03 9.9279916e-01]\n",
            " [9.9999684e-01 3.1401944e-06]\n",
            " [4.4706857e-01 5.5293143e-01]]\n"
          ]
        }
      ],
      "source": [
        "# find predictions for the test set\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "FZ8ZondzyeIK",
        "outputId": "bec6b167-34bd-48ff-e610-41fe9a9a5690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3a72876f0097>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# y_pred = np.argmax(y_pred, axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# y_test = np.argmax(y_test, axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# add labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ],
      "source": [
        "# calculate the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# y_pred = np.argmax(y_pred, axis=1)\n",
        "# y_test = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "# add labels \n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "FRVgTR8yyeIK",
        "outputId": "e6c3c32d-decb-4ec9-ab6c-477acdef2683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-d30f35c15a6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate f1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m     )\n\u001b[1;32m   1133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f-score\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m         \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     )\n\u001b[1;32m   1272\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ],
      "source": [
        "# calculate f1 score\n",
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, y_pred, average='macro')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmSPRkfQyeIL",
        "outputId": "fa37a96c-2dc9-49f3-b82c-29b416e6a94a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.4891526900754934"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate f1 score on random shuffle of y_pred\n",
        "from sklearn.utils import shuffle\n",
        "y_pred_shuffled = shuffle(y_pred)\n",
        "f1_score(y_test, y_pred_shuffled, average='macro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yJc-hiMyeIL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('bertLab')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad3b91568105f3920bc38a8c6d8f1e40ef9891f30a3db4c4d20c0f70c79e327e"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}