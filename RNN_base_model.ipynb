{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jacobhansen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://raw.githubusercontent.com/jacob-hansen/NLP_in_EHR_2022/910d9f0fcfeab083dff53ea2e2969c175cc816a0/train.csv'\n",
    "train_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#loading the english language small model of spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(sentence):\n",
    "    # lower case\n",
    "    sentence = sentence.lower()\n",
    "    # split by the label/sentence separator\n",
    "    sents = sentence.split(' val is ')\n",
    "    # seperate out the label and the next sentence\n",
    "    sents = [sents[0]] + [i for i in sents[1].split('. ')]\n",
    "    # remove any trailing whitespace\n",
    "    sents = [i.strip() for i in sents]\n",
    "    # remove stop words in every 1,3,5... sentence\n",
    "    # and apply tokenization\n",
    "    for i in range(0, len(sents), 2):\n",
    "        # remove stop words\n",
    "        sents[i] = [word for word in word_tokenize(sents[i]) if word not in sw_spacy]\n",
    "    # flatten sents \n",
    "    return_sents = []\n",
    "    for i in range(len(sents)):\n",
    "        if i % 2 == 0:\n",
    "            return_sents.extend(sents[i])\n",
    "        else:\n",
    "            return_sents.append(sents[i])\n",
    "    # repeat the label 3 times (loc = 1,3,5..) for every sentence \n",
    "    final_sents = []\n",
    "    for i in range(0, len(return_sents)):\n",
    "        if i % 2 == 0:\n",
    "            final_sents.append(return_sents[i])\n",
    "        else:\n",
    "            final_sents.append(return_sents[i])\n",
    "            final_sents.append(return_sents[i])\n",
    "            final_sents.append(return_sents[i])\n",
    "\n",
    "    return return_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the custom tokenizer to the dataframe\n",
    "train_df['tokenized'] = train_df['X_train'].apply(custom_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for training RNN\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df['y_train'].values\n",
    "target = to_categorical(target)\n",
    "# convert train_df['tokenized'] to a tensor\n",
    "# and pad the sequences to be the same length\n",
    "max_len = 200\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['tokenized'].values)\n",
    "X = tokenizer.texts_to_sequences(train_df['tokenized'].values)\n",
    "X = pad_sequences(X, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RNN model \n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "score, acc = model.evaluate(X_test, y_test, verbose = 2, batch_size = 64)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('bertLab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad3b91568105f3920bc38a8c6d8f1e40ef9891f30a3db4c4d20c0f70c79e327e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
